name: Double DQN Action Selection
description: Takes a state as JSON, trains a Double DQN model, and returns the best action.
inputs:
  - name: previous_state
    type: Path
    description: 'Path to a JSON file with the previous state'
  - name: current_state
    type: Path
    description: 'Path to a JSON file with the current state'
  - name: action_id
    type: Integer
    description: 'The action ID taken in the previous state'
  - name: episode
    type: Integer
    description: 'Current episode number for target network update tracking'
  - name: params_json
    type: String
    description: 'JSON array of parameter definitions, e.g. [{"key": "accuracy", "sign": "+", "mul": 1.0}]'
    optional: true
  - name: policy_model_weights
    type: Path
    description: 'Optional path to pre-trained policy model weights (.pt file)'
    optional: true
  - name: target_model_weights
    type: Path
    description: 'Optional path to pre-trained target model weights (.pt file)'
    optional: true
outputs:
  - name: best_action_name
    type: String
  - name: best_action_id
    type: String
  - name: next_episode
    type: String
  - name: updated_policy_model_weights
    type: Path
  - name: updated_target_model_weights
    type: Path
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet torch numpy || \
        python3 -m pip install --quiet torch numpy --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        import torch.optim as optim
        import random
        import json
        import argparse
        import os
        import numpy as np

        # Default parameters if not provided
        default_params = [
            {"key": "accuracy", "sign": "+", "mul": 1.0},
            {"key": "latency", "sign": "-", "mul": 0.5},
            {"key": "epochs", "sign": "-", "mul": 0.1}
        ]

        action_list = [
            {"id": 0, "name": "Action_A"},
            {"id": 1, "name": "Action_B"},
            {"id": 2, "name": "Action_C"},
            {"id": 3, "name": "Action_D"}
        ]

        # Improved deeper network (3. Model architecture)
        class QNetwork(nn.Module):
            def __init__(self, num_params, num_actions):
                super().__init__()
                self.fc1 = nn.Linear(num_params, 64)
                self.fc2 = nn.Linear(64, 64)
                self.fc3 = nn.Linear(64, num_actions)
            def forward(self, x):
                x = F.relu(self.fc1(x))
                x = F.relu(self.fc2(x))
                return self.fc3(x)

        def calculate_reward(state_values, action_id, params):
            reward = 0.0
            p_values = {p["key"]: val for p, val in zip(params, state_values)}

            for p in params:
                key = p["key"]
                sign = p["sign"]
                mul = p["mul"]
                
                if sign == '+':
                    reward += p_values[key] * mul
                elif sign == '-':
                    reward -= p_values[key] * mul
                else:
                    raise ValueError(f"Invalid sign '{sign}' for parameter '{key}'")

            return reward

        lr = 0.001
        gamma = 0.99
        TARGET_UPDATE_FREQ = 10

        parser = argparse.ArgumentParser()
        parser.add_argument('--previous_state', type=str, required=True)
        parser.add_argument('--current_state', type=str, required=True)
        parser.add_argument('--action_id', type=int, required=True)
        parser.add_argument('--episode', type=int, required=True)
        parser.add_argument('--params_json', type=str, default=json.dumps(default_params))
        parser.add_argument('--policy_model_weights', type=str, default=None)
        parser.add_argument('--target_model_weights', type=str, default=None)
        parser.add_argument('--best_action_name', type=str, required=True)
        parser.add_argument('--best_action_id', type=str, required=True)
        parser.add_argument('--next_episode', type=str, required=True)
        parser.add_argument('--updated_policy_model_weights', type=str, required=True)
        parser.add_argument('--updated_target_model_weights', type=str, required=True)
        args = parser.parse_args()

        params = json.loads(args.params_json)

        # (7) Reproducibility: fix random seeds
        seed = 42
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

        # (5) GPU support
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        policy_net = QNetwork(len(params), len(action_list)).to(device)
        target_net = QNetwork(len(params), len(action_list)).to(device)

        optimizer = optim.Adam(policy_net.parameters(), lr=lr)
        loss_fn = nn.MSELoss()

        # Load weights if available
        if args.policy_model_weights and os.path.exists(args.policy_model_weights):
            policy_net.load_state_dict(torch.load(args.policy_model_weights, map_location=device))
            print(f"Loaded policy model weights from {args.policy_model_weights}")
        else:
            print("No policy model weights provided or file not found. Starting with new policy network.")

        if args.target_model_weights and os.path.exists(args.target_model_weights):
            target_net.load_state_dict(torch.load(args.target_model_weights, map_location=device))
            print(f"Loaded target model weights from {args.target_model_weights}")
        else:
            target_net.load_state_dict(policy_net.state_dict())
            print("No target model weights provided or file not found. Initializing target network from policy network.")

        target_net.eval()

        def get_best_action(state, model, epsilon=0.0, print_q_values=True):
            if random.random() < epsilon:
                return random.randint(0, len(action_list) - 1)
            else:
                with torch.no_grad():
                    q_values = model(state)
                    if print_q_values:
                        probs = F.softmax(q_values, dim=1)
                        print("Action Probabilities:", probs.cpu().numpy())
                    return torch.argmax(q_values).item()

        def train_dqn_step(prev_state_dict, action_taken, new_state_dict, params):
            prev_ordered_state_values = [prev_state_dict[p["key"]] for p in params]
            prev_state_tensor = torch.tensor([prev_ordered_state_values], dtype=torch.float32, device=device)

            new_ordered_state_values = [new_state_dict[p["key"]] for p in params]
            new_state_tensor = torch.tensor([new_ordered_state_values], dtype=torch.float32, device=device)

            reward = calculate_reward(new_ordered_state_values, action_taken, params)

            q_values = policy_net(prev_state_tensor)

            with torch.no_grad():
                next_q_values_policy = policy_net(new_state_tensor)
                best_next_action = torch.argmax(next_q_values_policy, dim=1).item()
                next_q_value_target = target_net(new_state_tensor)[0, best_next_action]

            q_target = q_values.clone().detach()
            q_target[0, action_taken] = reward + gamma * next_q_value_target

            loss = loss_fn(q_values, q_target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # Single training step based on previous and current state
        print("\nStarting Single DQN Training Step...")
        
        with open(args.previous_state, 'r') as f:
            previous_state_dict = json.load(f)
        with open(args.current_state, 'r') as f:
            current_state_dict = json.load(f)

        action_taken = args.action_id
        episode = args.episode

        train_dqn_step(previous_state_dict, action_taken, current_state_dict, params)

        if episode % TARGET_UPDATE_FREQ == 0:
            target_net.load_state_dict(policy_net.state_dict())
            print(f"Episode {episode}: Target network updated.")

        # Save updated models
        torch.save(policy_net.state_dict(), args.updated_policy_model_weights)
        torch.save(target_net.state_dict(), args.updated_target_model_weights)
        print(f"Training step complete. Models saved to {args.updated_policy_model_weights} and {args.updated_target_model_weights}")

        # Inference on the current state
        current_state_tensor = torch.tensor([[current_state_dict[p["key"]] for p in params]], dtype=torch.float32, device=device)
        best_action_id_val = get_best_action(current_state_tensor, policy_net, epsilon=0.0, print_q_values=True)
        best_action_name_val = next((a["name"] for a in action_list if a["id"] == best_action_id_val), "Unknown")

        # Write outputs
        with open(args.best_action_id, 'w') as f:
            f.write(str(best_action_id_val))
        with open(args.best_action_name, 'w') as f:
            f.write(best_action_name_val)
        
        next_episode_val = episode + 1
        with open(args.next_episode, 'w') as f:
            f.write(str(next_episode_val))

        print(f"\nCurrent State: {current_state_dict}")
        print("Best Action ID:", best_action_id_val)
        print("Best Action Name:", best_action_name_val)
        print("Next Episode:", next_episode_val)

    args:
      - --previous_state
      - {inputPath: previous_state}
      - --current_state
      - {inputPath: current_state}
      - --action_id
      - {inputValue: action_id}
      - --episode
      - {inputValue: episode}
      - --params_json
      - {inputValue: params_json}
      - --policy_model_weights
      - {inputPath: policy_model_weights}
      - --target_model_weights
      - {inputPath: target_model_weights}
      - --best_action_name
      - {outputPath: best_action_name}
      - --best_action_id
      - {outputPath: best_action_id}
      - --next_episode
      - {outputPath: next_episode}
      - --updated_policy_model_weights
      - {outputPath: updated_policy_model_weights}
      - --updated_target_model_weights
      - {outputPath: updated_target_model_weights}
