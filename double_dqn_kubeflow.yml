name: Double DQN Action Selection
description: Takes a state as JSON, trains a Double DQN model, and returns the best action.
inputs:
  - name: previous_state
    type: Path
    description: 'Path to a JSON file with the previous state'
  - name: current_state
    type: Path
    description: 'Path to a JSON file with the current state'
  - name: action_id
    type: Integer
    description: 'The action ID taken in the previous state'
  - name: episode
    type: Integer
    description: 'Current episode number for target network update tracking'
  - name: params_json
    type: String
    description: 'JSON array of parameter definitions, e.g. [{"key": "accuracy", "sign": "+", "mul": 1.0}]'
    optional: true
  - name: policy_model_weights
    type: Path
    description: 'Optional path to pre-trained policy model weights (.pt file)'
    optional: true
  - name: target_model_weights
    type: Path
    description: 'Optional path to pre-trained target model weights (.pt file)'
    optional: true
  - name: rlaf_actions_json
    type: Path
    description: 'Path to a JSON file with the RLAF actions'
outputs:
  - name: best_action_name
    type: Path
  - name: best_action_id
    type: Integer
  - name: next_episode
    type: Integer
  - name: updated_policy_model_weights
    type: Path
  - name: updated_target_model_weights
    type: Path
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet torch numpy || \
        python3 -m pip install --quiet torch numpy --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        import torch.optim as optim
        import random
        import json
        import argparse
        import os
        import numpy as np

        # Default parameters if not provided
        default_params = [
            {"key": "accuracy", "sign": "+", "mul": 1.0},
            {"key": "latency", "sign": "-", "mul": 0.5},
            {"key": "epochs", "sign": "-", "mul": 0.1}
        ]

        parser = argparse.ArgumentParser()
        parser.add_argument('--previous_state', type=str, required=True)
        parser.add_argument('--current_state', type=str, required=True)
        parser.add_argument('--action_id', type=int, required=True)
        parser.add_argument('--episode', type=int, required=True)
        parser.add_argument('--params_json', type=str, default=json.dumps(default_params))
        parser.add_argument('--policy_model_weights', type=str, default=None)
        parser.add_argument('--target_model_weights', type=str, default=None)
        parser.add_argument('--rlaf_actions_json', type=str, required=True)
        parser.add_argument('--best_action_name', type=str, required=True)
        parser.add_argument('--best_action_id', type=str, required=True)
        parser.add_argument('--next_episode', type=str, required=True)
        parser.add_argument('--updated_policy_model_weights', type=str, required=True)
        parser.add_argument('--updated_target_model_weights', type=str, required=True)
        args = parser.parse_args()

        with open(args.rlaf_actions_json, 'r') as f:
            action_list_data = json.load(f)
        action_list = action_list_data['actions']

        # Improved deeper network (3. Model architecture)
        class QNetwork(nn.Module):
            def __init__(self, num_params, num_actions):
                super().__init__()
                self.fc1 = nn.Linear(num_params, 64)
                self.fc2 = nn.Linear(64, 64)
                self.fc3 = nn.Linear(64, num_actions)
            def forward(self, x):
                x = F.relu(self.fc1(x))
                x = F.relu(self.fc2(x))
                return self.fc3(x)

        def calculate_reward(state_values, action_id, params):
            reward = 0.0
            p_values = {p["key"]: val for p, val in zip(params, state_values)}

            for p in params:
                key = p["key"]
                sign = p["sign"]
                mul = p["mul"]
                
                if sign == '+':
                    reward += p_values[key] * mul
                elif sign == '-':
                    reward -= p_values[key] * mul
                else:
                    raise ValueError(f"Invalid sign '{sign}' for parameter '{key}'")

            return reward

        lr = 0.001
        gamma = 0.99
        TARGET_UPDATE_FREQ = 10

        print(f"Starting double_dqn_kubeflow component")
        print(f"Inputs:")
        print(f"  previous_state: {args.previous_state}")
        print(f"  current_state: {args.current_state}")
        print(f"  action_id: {args.action_id}")
        print(f"  episode: {args.episode}")
        print(f"  params_json: {args.params_json}")
        print(f"  policy_model_weights: {args.policy_model_weights}")
        print(f"  target_model_weights: {args.target_model_weights}")
        print(f"  rlaf_actions_json: {args.rlaf_actions_json}")

        params = json.loads(args.params_json)

        # (7) Reproducibility: fix random seeds
        seed = 42
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

        # (5) GPU support
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {device}")

        policy_net = QNetwork(len(params), len(action_list)).to(device)
        target_net = QNetwork(len(params), len(action_list)).to(device)

        optimizer = optim.Adam(policy_net.parameters(), lr=lr)
        loss_fn = nn.MSELoss()

        # Load weights if available
        if args.policy_model_weights and os.path.exists(args.policy_model_weights) and os.path.getsize(args.policy_model_weights) > 0:
            policy_net.load_state_dict(torch.load(args.policy_model_weights, map_location=device))
            print(f"Loaded policy model weights from {args.policy_model_weights}")
        else:
            print("No policy model weights provided or file is empty. Starting with new policy network.")

        if args.target_model_weights and os.path.exists(args.target_model_weights) and os.path.getsize(args.target_model_weights) > 0:
            target_net.load_state_dict(torch.load(args.target_model_weights, map_location=device))
            print(f"Loaded target model weights from {args.target_model_weights}")
        else:
            target_net.load_state_dict(policy_net.state_dict())
            print("No target model weights provided or file is empty. Initializing target network from policy network.")

        target_net.eval()

        def get_best_action(state, model, epsilon=0.0, print_q_values=True):
            if random.random() < epsilon:
                action = random.randint(0, len(action_list) - 1)
                print(f"Exploring: chose action {action} randomly.")
                return action
            else:
                with torch.no_grad():
                    q_values = model(state)
                    if print_q_values:
                        probs = F.softmax(q_values, dim=1)
                        print("Action Probabilities:", probs.cpu().numpy())
                    action = torch.argmax(q_values).item()
                    print(f"Exploiting: chose action {action} with highest Q-value.")
                    return action

        def train_dqn_step(prev_state_dict, action_taken, new_state_dict, params):
            prev_ordered_state_values = [prev_state_dict[p["key"]] for p in params]
            prev_state_tensor = torch.tensor([prev_ordered_state_values], dtype=torch.float32, device=device)

            new_ordered_state_values = [new_state_dict[p["key"]] for p in params]
            new_state_tensor = torch.tensor([new_ordered_state_values], dtype=torch.float32, device=device)

            reward = calculate_reward(new_ordered_state_values, action_taken, params)
            print(f"Calculated reward: {reward}")

            q_values = policy_net(prev_state_tensor)

            with torch.no_grad():
                next_q_values_policy = policy_net(new_state_tensor)
                best_next_action = torch.argmax(next_q_values_policy, dim=1).item()
                next_q_value_target = target_net(new_state_tensor)[0, best_next_action]

            q_target = q_values.clone().detach()
            q_target[0, action_taken] = reward + gamma * next_q_value_target

            loss = loss_fn(q_values, q_target)
            print(f"Calculated loss: {loss.item()}")
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # Single training step based on previous and current state
        print(f"Starting Single DQN Training Step")
        
        with open(args.previous_state, 'r') as f:
            previous_state_dict = json.load(f)
        with open(args.current_state, 'r') as f:
            current_state_dict = json.load(f)

        action_taken = args.action_id
        episode = args.episode

        if action_taken == -1:
            print(f"Initial Run Detected (action_id is -1)")
            print(f"Skipping training step and selecting a random action.")
            
            # Save the initialized (or loaded) models without updating them
            torch.save(policy_net.state_dict(), args.updated_policy_model_weights)
            torch.save(target_net.state_dict(), args.updated_target_model_weights)
            print(f"Passing through models without updates. Saved to {args.updated_policy_model_weights} and {args.updated_target_model_weights}")

            # Select a random action for the first step
            best_action_id_val = random.randint(0, len(action_list) - 1)
            print(f"Randomly selected action ID: {best_action_id_val}")

        else:
            train_dqn_step(previous_state_dict, action_taken, current_state_dict, params)

            if episode % TARGET_UPDATE_FREQ == 0 and episode > 0:
                target_net.load_state_dict(policy_net.state_dict())
                print(f"Episode {episode}: Target network updated.")

            # Save updated models
            torch.save(policy_net.state_dict(), args.updated_policy_model_weights)
            torch.save(target_net.state_dict(), args.updated_target_model_weights)
            print(f"Training step complete. Models saved to {args.updated_policy_model_weights} and {args.updated_target_model_weights}")

            # Inference on the current state
            print(f"Starting Inference on Current State")
            current_state_tensor = torch.tensor([[current_state_dict[p["key"]] for p in params]], dtype=torch.float32, device=device)
            best_action_id_val = get_best_action(current_state_tensor, policy_net, epsilon=0.0, print_q_values=True)
        best_action_name_val = next((a["name"] for a in action_list if a["id"] == best_action_id_val), "Unknown")

        # Write outputs
        with open(args.best_action_id, 'w') as f:
            f.write(str(best_action_id_val))
        with open(args.best_action_name, 'w') as f:
            f.write(best_action_name_val)
        
        next_episode_val = episode + 1
        with open(args.next_episode, 'w') as f:
            f.write(str(next_episode_val))

        print(f"double_dqn_kubeflow component finished")
        print(f"Outputs created:")
        print(f"  best_action_name: {args.best_action_name} -> {best_action_name_val}")
        print(f"  best_action_id: {args.best_action_id} -> {best_action_id_val}")
        print(f"  next_episode: {args.next_episode} -> {next_episode_val}")
        print(f"  updated_policy_model_weights: {args.updated_policy_model_weights}")
        print(f"  updated_target_model_weights: {args.updated_target_model_weights}")

    args:
      - --previous_state
      - {inputPath: previous_state}
      - --current_state
      - {inputPath: current_state}
      - --action_id
      - {inputValue: action_id}
      - --episode
      - {inputValue: episode}
      - --params_json
      - {inputValue: params_json}
      - --policy_model_weights
      - {inputPath: policy_model_weights}
      - --target_model_weights
      - {inputPath: target_model_weights}
      - --rlaf_actions_json
      - {inputPath: rlaf_actions_json}
      - --best_action_name
      - {outputPath: best_action_name}
      - --best_action_id
      - {outputPath: best_action_id}
      - --next_episode
      - {outputPath: next_episode}
      - --updated_policy_model_weights
      - {outputPath: updated_policy_model_weights}
      - --updated_target_model_weights
      - {outputPath: updated_target_model_weights}
