name: Preprocess Data
description: Processes raw data and creates training and testing dataloaders.
inputs:
  - {name: json_data, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: train_loader, type: Dataset}
  - {name: test_loader, type: Dataset}
  - {name: input_dim, type: String}
  - {name: processed_data, type: Dataset}
implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import numpy as np
        import pandas as pd
        import torch
        from sklearn.preprocessing import MinMaxScaler
        from torch.utils.data import TensorDataset, DataLoader

        def process_total_metrics_data(json_data):
            records = []
            for item in json_data:
                record = {
                    'totalallocatable.cpu': float(item['totalallocatable']['cpu'].replace('c', '')),
                    'totalallocatable.memory': float(item['totalallocatable']['memory'].replace('TB', '')) * 1e12,
                    'totalallocatable.storage': float(item['totalallocatable']['storage'].replace('TB', '')) * 1e12,
                    'timestamp': item['timestamp']
                }
                records.append(record)
            df = pd.DataFrame(records)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values(by='timestamp')
            return df

        def create_sequences_with_labels(data, labels, seq_length):
            X, y = [], []
            for i in range(len(data) - seq_length):
                X.append(data[i:i+seq_length])
                y.append(labels[i+seq_length-1])
            return np.array(X), np.array(y)

        parser = argparse.ArgumentParser()
        parser.add_argument('--json_data', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--train_loader', type=str, required=True)
        parser.add_argument('--test_loader', type=str, required=True)
        parser.add_argument('--input_dim', type=str, required=True)
        parser.add_argument('--processed_data', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)

        with open(args.json_data, 'rb') as f:
            json_data = pickle.load(f)
        print(f"Loaded json_data. Length: {len(json_data)}")

        if config['use_case'] == "Fine-Grained Build Cost Metering Loops":
            df = process_total_metrics_data(json_data)
        else:
            df = pd.DataFrame(json_data)
        print(f"Created DataFrame. Shape: {df.shape}")

        all_columns = config['feature_columns'] + [config['target_column']]
        for col in all_columns:
            if '.' not in col:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        df.dropna(subset=all_columns, inplace=True)
        print(f"DataFrame shape after dropping NA: {df.shape}")

        features = config['feature_columns']
        scaler = MinMaxScaler()
        data_for_sequencing = scaler.fit_transform(df[features].astype(float).values)
        y_labels = df[config['target_column']].astype(float).values

        X, y = create_sequences_with_labels(data_for_sequencing, y_labels, config['seq_length'])
        print(f"Created sequences. X shape: {X.shape}, y shape: {y.shape}")
        
        permutation = np.random.permutation(len(X))
        X = X[permutation]
        y = y[permutation]

        train_size = int(len(X) * 0.8)
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]
        print(f"Train data shape: {X_train.shape}, Test data shape: {X_test.shape}")

        X_train = torch.tensor(X_train, dtype=torch.float32)
        y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
        X_test = torch.tensor(X_test, dtype=torch.float32)
        y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)

        train_dataset = TensorDataset(X_train, y_train)
        test_dataset = TensorDataset(X_test, y_test)

        train_loader_obj = DataLoader(train_dataset, batch_size=config.get('batch_size', 32), shuffle=True)
        test_loader_obj = DataLoader(test_dataset, batch_size=config.get('batch_size', 32), shuffle=False)

        os.makedirs(os.path.dirname(args.train_loader), exist_ok=True)
        with open(args.train_loader, "wb") as f:
            pickle.dump(train_loader_obj, f)

        os.makedirs(os.path.dirname(args.test_loader), exist_ok=True)
        with open(args.test_loader, "wb") as f:
            pickle.dump(test_loader_obj, f)

        input_dim = X_train.shape[2]
        os.makedirs(os.path.dirname(args.input_dim), exist_ok=True)
        with open(args.input_dim, "w") as f:
            f.write(str(input_dim))

        class DataWrapper:
            def __init__(self, train_loader, test_loader, input_dim):
                self.train_loader = train_loader
                self.test_loader = test_loader
                self.input_dim = input_dim
        
        data_to_save = DataWrapper(train_loader_obj, test_loader_obj, input_dim)
        os.makedirs(os.path.dirname(args.processed_data), exist_ok=True)
        with open(args.processed_data, "wb") as f:
            pickle.dump(data_to_save, f)

        print(f"Saved train_loader to {args.train_loader}")
        print(f"Saved test_loader to {args.test_loader}")
        print(f"Saved input_dim to {args.input_dim}")
        print(f"Saved processed_data to {args.processed_data}")
    args:
      - --json_data
      - {inputPath: json_data}
      - --config
      - {inputValue: config}
      - --train_loader
      - {outputPath: train_loader}
      - --test_loader
      - {outputPath: test_loader}
      - --input_dim
      - {outputPath: input_dim}
      - --processed_data
      - {outputPath: processed_data}
