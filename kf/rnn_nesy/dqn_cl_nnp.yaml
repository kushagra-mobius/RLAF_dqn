name: DQN RLAF Loop
description: Triggers the DQN RLAF pipeline in a loop to optimize model hyperparameters, controlled by a pierce_or_not flag.
inputs:
  - {name: trained_model, type: Model}
  - {name: init_metrics, type: Metrics}
  - {name: data_path, type: Dataset}
  - {name: config, type: String}
  - {name: domain, type: String}
  - {name: schema_id, type: String}
  - {name: model_id, type: String}
  - {name: dqn_pipeline_id, type: String}
  - {name: pipeline_domain, type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: access_token, type: string}
  - {name: tasks, type: Dataset}
outputs:
  - {name: rlaf_output, type: Dataset}
  - {name: retrained_model, type: Model}
implementation:
  container:
    image: kushagra4761/nesy-factory-library
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import time
        import pickle
        from typing import List, Dict, Any
        import numpy as np
        from nesy_factory.RNNs.simple_rnn import SimpleRNN
        from nesy_factory.RNNs.gru import GRU
        from torch.utils.data import TensorDataset, DataLoader

        class DataWrapper:
          def __init__(self, data_dict):
              self.__dict__.update(data_dict)

        class StandardScaler():
            def __init__(self, mean, std):
                self.mean = mean
                self.std = std
            def transform(self, data):
                return (data - self.mean) / self.std
            def inverse_transform(self, data):
                return (data * self.std) + self.mean
        
        class TemporalDataSplitter:
          
          def __init__(self, data, config, strategy='temporal_split'):
              self.data = data
              self.config = config
              self.strategy = strategy
              
          def create_continual_tasks(self, num_tasks: int = 3) -> List[Dict]:
              return self._temporal_split(num_tasks)
          
          def _temporal_split(self, num_tasks: int) -> List[Dict]:
              tasks = []
              
              train_size = len(self.data['X_train'])
              test_size = len(self.data['X_test'])
              
              train_splits = np.array_split(range(train_size), num_tasks)
              test_splits = np.array_split(range(test_size), num_tasks)
              
              for i in range(num_tasks):
                  task_data = {
                      'task_id': i,
                      'X_train': self.data['X_train'][train_splits[i]],
                      'y_train': self.data['y_train'][train_splits[i]],
                      'X_test': self.data['X_test'][test_splits[i]],
                      'y_test': self.data['y_test'][test_splits[i]],
                      'description': f'Temporal Period {i+1}/{num_tasks}'
                  }
                  
                  train_dataset = TensorDataset(torch.tensor(task_data['X_train'], dtype=torch.float32), torch.tensor(task_data['y_train'], dtype=torch.float32).unsqueeze(1))
                  test_dataset = TensorDataset(torch.tensor(task_data['X_test'], dtype=torch.float32), torch.tensor(task_data['y_test'], dtype=torch.float32).unsqueeze(1))

                  task_data['train_loader'] = DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True)
                  task_data['test_loader'] = DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False)
                  
                  tasks.append(task_data)
              
              return tasks

        class ContinualRNNTrainer:
            
            def __init__(self, config: Dict[str, Any]):
                self.config = config
                self.results = {}
                
            def train_continual_rnn(
                self, 
                tasks: List[Dict], 
                model, 
                strategies: List[str] = ['naive', 'replay', 'regularized']
            ) -> Dict[str, Any]:
                results = {}
                
                for strategy_name in strategies:
                    print(f'Training with {strategy_name.upper()} strategy')
                    strategy_results = self._train_single_strategy(tasks, strategy_name, model)
                    results[strategy_name] = strategy_results
                    
                return results
            
            def _train_single_strategy(self, tasks: List[Dict], strategy_name: str, model) -> Dict[str, Any]:
                
                task_metrics = []
                all_task_performance = []
                previous_task_data = []
                
                print(f" Learning {len(tasks)} sequential tasks")
                
                for task_idx, task_data in enumerate(tasks):
                    print(f"Learning Task {task_idx + 1}: {task_data['description']}")
                    
                    if strategy_name == 'naive':
                        training_loader = task_data['train_loader']
                    elif strategy_name == 'replay':
                        if previous_task_data:
                            training_loader = self._create_replay_loader(task_data, previous_task_data)
                        else:
                            training_loader = task_data['train_loader']
                    elif strategy_name == 'regularized':
                        training_loader = task_data['train_loader']
                        if task_idx > 0:
                            self._store_important_params(model)
                    else:
                        training_loader = task_data['train_loader']
                    
                    print(f"  Training on Task {task_idx + 1}")
                    for epoch in range(self.config.get('epochs', 10)):
                        model.train()
                        total_loss = 0
                        for inputs, labels in training_loader:
                            if strategy_name == 'regularized' and task_idx > 0:
                                loss = self._train_with_regularization(model, (inputs, labels))
                            else:
                                loss = model.train_step((inputs, labels))
                            total_loss += loss
                        avg_loss = total_loss / len(training_loader)
                        if epoch % 5 == 0:
                            print(f"    Epoch {epoch:03d} | Loss: {avg_loss:.4f}")
                    
                    if strategy_name == 'replay':
                        previous_task_data.append(task_data)
                        if len(previous_task_data) > 3:
                            previous_task_data = previous_task_data[-3:]
                    
                    current_task_metrics = model.eval_step(task_data['test_loader'])
                    task_metrics.append(current_task_metrics)
                    
                    print(f"     Task {task_idx + 1} Test Loss: {current_task_metrics['loss']:.4f}")
                    
                    task_performance = []
                    for eval_task_idx in range(task_idx + 1):
                        eval_metrics = model.eval_step(tasks[eval_task_idx]['test_loader'])
                        task_performance.append({
                            'task_id': eval_task_idx,
                            'loss': eval_metrics['loss'],
                            'description': tasks[eval_task_idx]['description']
                        })
                    
                    all_task_performance.append(task_performance)
                    
                    if task_idx > 0:
                        print(f"Performance on previous tasks:")
                        for prev_task in task_performance[:-1]:
                            print(f"    Task {prev_task['task_id'] + 1}: {prev_task['loss']:.4f}")
                
                cl_metrics = self._calculate_continual_metrics(all_task_performance)
                
                final_eval_metrics = [model.eval_step(tasks[i]['test_loader']) for i in range(len(tasks))]
                
                avg_metrics = {}
                if final_eval_metrics:
                    for key in final_eval_metrics[0]:
                        avg_metrics[key] = np.mean([m[key] for m in final_eval_metrics])

                results = {
                    'strategy': strategy_name,
                    'task_metrics': task_metrics,
                    'all_task_performance': all_task_performance,
                    'continual_metrics': cl_metrics,
                    'final_model': model,
                    'average_eval_metrics': avg_metrics
                }
                
                return results
            
            def _create_replay_loader(self, current_task: Dict, previous_tasks: List[Dict]) -> DataLoader:
                replay_ratio = 0.3
                
                current_X = current_task['X_train']
                current_y = current_task['y_train']
                current_size = len(current_X)
                replay_size = int(current_size * replay_ratio / (1 - replay_ratio))
                
                replay_x = []
                replay_y = []
                
                for prev_task in previous_tasks:
                    if len(prev_task['X_train']) > 0:
                        sample_size = min(replay_size // len(previous_tasks), len(prev_task['X_train']))
                        if sample_size > 0:
                            indices = np.random.choice(len(prev_task['X_train']), sample_size, replace=False)
                            replay_x.append(prev_task['X_train'][indices])
                            replay_y.append(prev_task['y_train'][indices])
                
                if replay_x:
                    combined_x = np.concatenate([current_X] + replay_x)
                    combined_y = np.concatenate([current_y] + replay_y)
                else:
                    combined_x = current_X
                    combined_y = current_y
                
                replay_dataset = TensorDataset(torch.tensor(combined_x, dtype=torch.float32), torch.tensor(combined_y, dtype=torch.float32).unsqueeze(1))
                return DataLoader(replay_dataset, batch_size=self.config['batch_size'], shuffle=True)

            def _store_important_params(self, model):
                if not hasattr(self, 'important_params'):
                    self.important_params = {}
                
                for name, param in model.named_parameters():
                    if param.requires_grad:
                        self.important_params[name] = param.clone().detach()
            
            def _train_with_regularization(self, model, data) -> float:
                base_loss = model.train_step(data)
                
                reg_loss = 0.0
                reg_lambda = 0.1
                
                if hasattr(self, 'important_params'):
                    for name, param in model.named_parameters():
                        if name in self.important_params and param.requires_grad:
                            reg_loss += torch.sum((param - self.important_params[name]).pow(2))
                
                total_loss = base_loss + reg_lambda * reg_loss
                
                model.optimizer.zero_grad()
                total_loss.backward()
                model.optimizer.step()

                return total_loss.item()
            
            def _calculate_continual_metrics(self, all_task_performance: List[List[Dict]]) -> Dict[str, float]:
                
                final_performance = all_task_performance[-1]
                average_loss = np.mean([task['loss'] for task in final_performance])
                
                backward_transfers = []
                if len(all_task_performance) > 1:
                    for task_idx in range(len(all_task_performance) - 1):
                        initial_loss = all_task_performance[task_idx][task_idx]['loss']
                        final_loss = all_task_performance[-1][task_idx]['loss']
                        backward_transfer = initial_loss - final_loss # Lower loss is better
                        backward_transfers.append(backward_transfer)
                
                avg_backward_transfer = np.mean(backward_transfers) if backward_transfers else 0.0
                
                forgetting_scores = []
                for task_idx in range(len(all_task_performance) - 1):
                    min_loss = all_task_performance[task_idx][task_idx]['loss']
                    final_loss = all_task_performance[-1][task_idx]['loss']
                    forgetting = final_loss - min_loss
                    forgetting_scores.append(max(0, forgetting))
                
                avg_forgetting = np.mean(forgetting_scores) if forgetting_scores else 0.0
                
                return {
                    'average_loss': average_loss,
                    'backward_transfer': avg_backward_transfer,
                    'forgetting': avg_forgetting,
                    'num_tasks': len(all_task_performance)
                }
        #  API/DB Helper Functions
        def get_retry_session():
            retry_strategy = Retry(
                total=5,
                status_forcelist=[500, 502, 503, 504],
                backoff_factor=1
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session

        def trigger_pipeline(config, pipeline_domain, dqn_params=None):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
            pipeline_params = {"param_json": json.dumps(dqn_params)} if dqn_params else {}
            payload = json.dumps({
                "pipelineType": "ML", "containerResources": {}, "experimentId": config['experiment_id'],
                "enableCaching": True, "parameters": pipeline_params, "version": 1
            })
            print(f"{payload}")
            headers = {
                'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}",
                'Content-Type': 'application/json'
            }
            print(f"Trigger pipeline request URL: {url}, Headers: {headers}, Payload: {payload}")
            response = http.post(url, headers=headers, data=payload, timeout=30)
            response.raise_for_status()
            print(f"Triggered pipeline. Status Code: {response.status_code}, Response: {response.json()}")
            return response.json()['runId']

        def get_pipeline_status(config, pipeline_domain):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
            headers = {'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}"}
            print(f"Get pipeline status request URL: {url}, Headers: {headers}")
            response = http.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            pipeline_status = response.json()
            print(f"Get pipeline status. Status Code: {response.status_code}, Full response: {pipeline_status}")
            latest_state = pipeline_status['run_details']['state_history'][-1]
            return latest_state['state']

        def get_instance(access_token, domain, schema_id, model_id):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {"dbType": "TIDB", "ownedOnly": True, "filter": {"model_id": model_id}}
            print(f"Get instance request URL: {url}, Headers: {headers}, Payload: {payload}")
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            print(f"Get instance. Status Code: {response.status_code}, Full response: {response.json()}")
            return response.json()['content'][0]

        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {"conditions": [{"field": "model_id", "operator": "EQUAL", "value": model_id}]},
                "partialUpdateRequests": [{"patch": [{"operation": "REPLACE", "path": f"{field}", "value": value}]}]
            }
            print(f"Update instance field request - URL: {url}, Headers: {headers}, Payload: {json.dumps(payload)}")
            headers_str = " ".join([f"-H '{k}: {v}'" for k, v in headers.items()])
            print(f"curl -X PATCH '{url}' {headers_str} -d '{json.dumps(payload)}'")
            response = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
            print(f"Update instance field. Status Code: {response.status_code}, Full response: {response.json()}")
            print(f"Successfully updated field '{field}' for model_id '{model_id}'")

        #  Core Logic 
        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, dqn_params):
            run_id = trigger_pipeline(config, pipeline_domain, dqn_params)
            config["run_id"] = run_id
            while True:
                status = get_pipeline_status(config, pipeline_domain)
                print(f"Current DQN pipeline status: {status}")
                if status == 'SUCCEEDED':
                    print("DQN Pipeline execution completed.")
                    break
                elif status in ['FAILED', 'ERROR']:
                    raise RuntimeError(f"DQN Pipeline failed with status {status}")
                time.sleep(60)

        def model_retraining(action, model_path, data_path, config, tasks_path, output_model_path, previous_metrics, dqn_params):
            with open(data_path, "rb") as f: data = pickle.load(f)
            with open(tasks_path, "rb") as f: tasks = pickle.load(f)
            
            config.update(action)
            
            model_config = {
                'input_dim': len(config['feature_columns']),
                'hidden_dim': config['hidden_dim'],
                'output_dim': 1,
                'num_layers': config['num_layers'],
                'dropout': config['dropout'],
                'optimizer': 'adam',
                'learning_rate': config['learning_rate'],
                'epochs': config['epochs'],
                'loss_function': config['loss_function']
            }

            if config['model_type'] == 'SimpleRNN':
                model = SimpleRNN(model_config)
            elif config['model_type'] == 'GRU':
                model = GRU(model_config)
            else:
                raise ValueError("Invalid model type specified")

            model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
            model._init_optimizer_and_criterion()

            print("Starting continual learning experiments")
            trainer = ContinualRNNTrainer(config)
            continual_strategies = ['naive']
            
            results = trainer.train_continual_rnn(tasks=tasks, strategies=continual_strategies, model=model)
            print(f"{results}")
            
            average_eval_metrics = results['naive']['average_eval_metrics']
            
            improvement_score = 0
            for param in dqn_params:
                key = param['key']
                sign = 1 if param['sign'] == '+' else -1
                if key in average_eval_metrics and key in previous_metrics:
                    improvement = (average_eval_metrics[key] - previous_metrics[key]) * sign
                    improvement_score += improvement

            if improvement_score > 0:
                print(f"Metrics improved (score: {improvement_score:.4f}). Saving model.")
                os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                final_model = results['naive']['final_model']
                torch.save(final_model.state_dict(), output_model_path)
                print(f"Saved retrained model to {output_model_path}")
            else:
                print(f"No improvement in metrics (score: {improvement_score:.4f}). Model not saved.")
                os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                with open(output_model_path, 'w') as f:
                    f.write("Model not saved due to lack of improvement.")

            return {"metrics": average_eval_metrics, "model_path": output_model_path}


        #  Main Execution
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()

            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            with open(args.init_metrics, 'r') as f:
                current_metrics = json.load(f)
 
            action_id_for_next_pierce = -1
 
            for i in range(2):
                print(f" RLAF Loop Iteration ")
                
                cleaned_metrics = {}
                dqn_params = []
                for key, value in current_metrics.items():
                    try:
                        cleaned_metrics[key] = float(value)
                        if "loss" in key.lower() or "accuracy" in key.lower() or "mse" in key.lower() or "mae" in key.lower():
                            sign = "+" if "accuracy" in key.lower() or "f1" in key.lower() else "-"
                            dqn_params.append({"key": key, "sign": sign, "mul": 1.0})
                    except (ValueError, TypeError):
                        print(f"Warning: Could not convert metric '{key}' with value '{value}' to float. Skipping.")
                
                print(f"Dynamically generated param_json for DQN: {json.dumps(dqn_params)}")

                instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                print(f"{instance}")
                if instance.get('pierce2rlaf'):
                    latest_pierce2rlaf = instance['pierce2rlaf'][-1]
                    previous_state = latest_pierce2rlaf['current_state']
                    episode = latest_pierce2rlaf['episode']
                else:
                    previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                    episode = 0
 
                new_pierce2rlaf_entry = {
                    "action_id": action_id_for_next_pierce, "previous_state": previous_state,
                    "current_state": cleaned_metrics, "episode": episode, "timestamp": int(time.time())
                }
                pierce2rlaf_history = instance.get("pierce2rlaf", [])
                pierce2rlaf_history.append(new_pierce2rlaf_entry)
                update_instance_field(access_token, args.domain, args.schema_id, args.model_id, "pierce2rlaf", pierce2rlaf_history)

                dqn_config = {
                    "pipeline_id": args.dqn_pipeline_id, "experiment_id": args.dqn_experiment_id, "access_token": access_token
                }
                print(f"{dqn_config}")
                trigger_and_wait_for_dqn_pipeline(dqn_config, args.pipeline_domain, dqn_params)

                updated_instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                latest_rlaf2pierce = updated_instance['rlaf2pierce'][-1]
                
                if not latest_rlaf2pierce.get("pierce_or_not", True):
                    print("pierce_or_not is false. Exiting loop.")
                    break
                print(f"{latest_rlaf2pierce}")
                rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                action_id_for_next_pierce = latest_rlaf2pierce['action_id']
                action_details = next((a for a in rlaf_actions if a["id"] == action_id_for_next_pierce), None)
                if not action_details:
                    raise ValueError(f"Action with ID {action_id_for_next_pierce} not found in rlaf_actions")
 
                print(f"DQN pipeline recommended action: {action_details}. Retraining model.")
                retraining_results = model_retraining(
                    action_details['params'], args.trained_model, args.data_path, json.loads(args.config), args.tasks,
                    args.retrained_model, previous_state, dqn_params
                )
                current_metrics = retraining_results["metrics"]

            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            with open(args.rlaf_output, 'w') as f:
                json.dump({"final_metrics": current_metrics}, f, indent=4)
            print(f"RLAF loop finished. Final parameters written to {args.rlaf_output}")

        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}