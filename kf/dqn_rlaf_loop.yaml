name: DQN RLAF Loop
description: Triggers the DQN RLAF pipeline in a loop to optimize model hyperparameters, controlled by a pierce_or_not flag.
inputs:
  - {name: trained_model, type: Model}
  - {name: init_metrics, type: Metrics}
  - {name: data_path, type: Dataset}
  - {name: config, type: String}
  - {name: domain, type: String}
  - {name: schema_id, type: String}
  - {name: model_id, type: String}
  - {name: dqn_pipeline_id, type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: access_token, type: string}
outputs:
  - {name: rlaf_output, type: Dataset}
implementation:
  container:
    image: nikhilv215/nesy-factory:v2
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        import time
        import pickle
        from nesy_factory.GNNs import create_model

        # --- API/DB Helper Functions ---
        def trigger_pipeline(config, params_json_str=None):
            url = f"https://ig.mobiusdtaas.ai/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
            pipeline_params = {"params_json": params_json_str} if params_json_str else {}
            payload = json.dumps({
                "pipelineType": "ML", "containerResources": {}, "experimentId": config['experiment_id'],
                "enableCaching": True, "parameters": pipeline_params, "version": 1
            })
            headers = {
                'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}",
                'Content-Type': 'application/json'
            }
            response = requests.post(url, headers=headers, data=payload)
            response.raise_for_status()
            print(f"Triggered pipeline. Response: {response.json()}")
            return response.json()['runId']

        def get_pipeline_status(config):
            url = f"https://ig.mobiusdtaas.ai/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
            headers = {'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}"}
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            pipeline_status = response.json()
            latest_state = pipeline_status['run_details']['state_history'][-1]
            return latest_state['state']

        def get_instance(access_token, domain, schema_id, model_id):
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {"dbType": "TIDB", "ownedOnly": True, "filter": {"model_id": model_id}}
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            return response.json()['content'][0]

        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {"conditions": [{"field": "model_id", "operator": "EQUAL", "value": model_id}]},
                "partialUpdateRequests": [{"patch": [{"operation": "REPLACE", "path": field, "value": value}]}]
            }
            response = requests.patch(url, headers=headers, json=payload)
            response.raise_for_status()
            print(f"Successfully updated field '{field}' for model_id '{model_id}'")

        # --- Core Logic ---
        def trigger_and_wait_for_dqn_pipeline(config, params_json_str):
            run_id = trigger_pipeline(config, params_json_str)
            config["run_id"] = run_id
            while True:
                status = get_pipeline_status(config)
                print(f"Current DQN pipeline status: {status}")
                if status == 'SUCCEEDED':
                    print("DQN Pipeline execution completed.")
                    break
                elif status in ['FAILED', 'ERROR']:
                    raise RuntimeError(f"DQN Pipeline failed with status {status}")
                time.sleep(60)

        def model_retraining(action, model_path, data_path, config_path):
            print(f"Starting model retraining with action: {action}")
            with open(data_path, "rb") as f: data = pickle.load(f)
            with open(config_path, 'r') as f: config = json.load(f)
            config.update(action)
            print(f"Updated training config: {config}")
            model_name = config.get('model_name', 'tgcn')
            model = create_model(model_name, config)
            model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
            epochs = config.get('epochs', 10)
            for epoch in range(epochs):
                loss = model.train_step(data, data.train_mask)
                if epoch % 10 == 0: print(f"Epoch {epoch:03d} | Loss: {loss:.4f}")
            torch.save(model.state_dict(), model_path)
            eval_metrics = model.eval_step(data, data.test_mask)
            print(f"Retraining complete. Eval metrics: {eval_metrics}")
            cleaned_metrics = {}
            for key, value in eval_metrics.items():
                try:
                    cleaned_metrics[key] = float(value)
                except (ValueError, TypeError):
                    print(f"Warning: Could not convert metric '{key}' with value '{value}' to float. Skipping.")
            return cleaned_metrics

        # --- Main Execution ---
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            args = parser.parse_args()

            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            with open(args.init_metrics, 'r') as f:
                current_metrics = json.load(f)

            for i in range(2): # Hardcoded loop limit of 2
                print(f"--- RLAF Loop Iteration {i+1}/2 ---")
                
                cleaned_metrics = {}
                dqn_params = []
                for key, value in current_metrics.items():
                    try:
                        cleaned_metrics[key] = float(value)
                        sign = "+" if "accuracy" in key.lower() or "f1" in key.lower() else "-"
                        dqn_params.append({"key": key, "sign": sign, "mul": 1.0})
                    except (ValueError, TypeError):
                        print(f"Warning: Could not convert metric '{key}' with value '{value}' to float. Skipping.")
                
                params_json_str = json.dumps(dqn_params)
                print(f"Dynamically generated params_json for DQN: {params_json_str}")

                instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                
                if instance.get('pierce2rlaf'):
                    latest_pierce2rlaf = instance['pierce2rlaf'][-1]
                    previous_state = latest_pierce2rlaf['current_state']
                    action_id = latest_pierce2rlaf['action_id']
                    episode = latest_pierce2rlaf['episode']
                else:
                    previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                    action_id = -1
                    episode = 0

                new_pierce2rlaf_entry = {
                    "action_id": action_id, "previous_state": previous_state,
                    "current_state": cleaned_metrics, "episode": episode, "timestamp": int(time.time())
                }
                pierce2rlaf_history = instance.get("pierce2rlaf", [])
                pierce2rlaf_history.append(new_pierce2rlaf_entry)
                update_instance_field(access_token, args.domain, args.schema_id, args.model_id, "pierce2rlaf", pierce2rlaf_history)

                dqn_config = {
                    "pipeline_id": args.dqn_pipeline_id, "experiment_id": args.dqn_experiment_id, "access_token": access_token
                }
                trigger_and_wait_for_dqn_pipeline(dqn_config, params_json_str)

                updated_instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                latest_rlaf2pierce = updated_instance['rlaf2pierce'][-1]
                
                if not latest_rlaf2pierce.get("pierce_or_not", True):
                    print("pierce_or_not is false. Exiting loop.")
                    break

                rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                best_action_id = latest_rlaf2pierce['action_id']
                action_details = next((a for a in rlaf_actions if a["id"] == best_action_id), None)
                if not action_details:
                    raise ValueError(f"Action with ID {best_action_id} not found in rlaf_actions")

                print(f"DQN pipeline recommended action: {action_details['name']}. Retraining model.")
                current_metrics = model_retraining(action_details['params'], args.trained_model, args.data_path, args.config)

            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            with open(args.rlaf_output, 'w') as f:
                json.dump({"final_metrics": current_metrics}, f, indent=4)
            print(f"RLAF loop finished. Final parameters written to {args.rlaf_output}")

        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputPath: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}