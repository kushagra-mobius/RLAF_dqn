name: GRPO Training
description: Trains a model using GRPO.
inputs:
  - {name: config, type: String}
  - {name: dataset, type: Dataset}
outputs:
  - {name: trained_model, type: Model}
implementation:
  container:
    image: kushagra4761/nesy-factory-library
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import torch
        import re
        import numpy as np
        import pandas as pd
        from datasets import load_dataset, Dataset
        from unsloth import FastLanguageModel
        from vllm import SamplingParams
        from trl import GRPOConfig, GRPOTrainer
        from transformers import TextStreamer
        import argparse
        import json
        import pickle

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--dataset', type=str, required=True)
            parser.add_argument('--trained_model', type=str, required=True)
            args = parser.parse_args()

            config = json.loads(args.config)

            with open(args.dataset, 'rb') as f:
                dataset = pickle.load(f)

            # Model and Tokenizer setup
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=config["model_name"],
                max_seq_length=config["max_seq_length"],
                load_in_4bit=False,
                fast_inference=True,
                max_lora_rank=config["lora_rank"],
                gpu_memory_utilization=0.9,
            )

            # Chat Template setup
            system_prompt = f"""You are given a problem.
        Think about the problem and provide your working out.
        Place it between {config["reasoning_start"]} and {config["reasoning_end"]}.
        Then, provide your solution between {config["solution_start"]}{config["solution_end"]}"""

            chat_template = \
                "{% if messages[0]['role'] == 'system' %}"\
                    "{{ messages[0]['content'] + eos_token }}"\
                    "{% set loop_messages = messages[1:] %}"\
                "{% else %}"\
                    "{{ '{system_prompt}' + eos_token }}"\
                    "{% set loop_messages = messages %}"\
                "{% endif %}"\
                "{% for message in loop_messages %}"\
                    "{% if message['role'] == 'user' %}"\
                        "{{ message['content'] }}"\
                    "{% elif message['role'] == 'assistant' %}"\
                        "{{ message['content'] + eos_token }}"\
                    "{% endif %}"\
                "{% endfor %}"\
                "{% if add_generation_prompt %}{{ '{reasoning_start}' }}"\
                "{% endif %}"

            chat_template = chat_template\
                .replace("'{system_prompt}'", f"'{system_prompt}'")\
                .replace("'{reasoning_start}'", f"'{config['reasoning_start']}'")
            tokenizer.chat_template = chat_template

            # Reward functions and regex
            solution_end_regex = r"</SOLUTION>[\s]{0,}" + \
                "(?:" + re.escape(tokenizer.eos_token) + ")?"

            match_format = re.compile(
                rf"{config['reasoning_end']}.*?"\
                rf"{config['solution_start']}(.+?){solution_end_regex}"\
                rf"[\s]{{0,}}$",
                flags=re.MULTILINE | re.DOTALL
            )

            match_numbers = re.compile(
                config['solution_start'] + r".*?[\s]{0,}([-]?[\d\.\,]{1,})",
                flags=re.MULTILINE | re.DOTALL
            )

            def match_format_exactly(completions, **kwargs):
                scores = []
                for completion in completions:
                    score = 0
                    response = completion[0]["content"]
                    if match_format.search(response) is not None:
                        score += 3.0
                    scores.append(score)
                return scores

            def match_format_approximately(completions, **kwargs):
                scores = []
                for completion in completions:
                    score = 0
                    response = completion[0]["content"]
                    score += 0.5 if response.count(config['reasoning_end']) == 1 else -1.0
                    score += 0.5 if response.count(config['solution_start']) == 1 else -1.0
                    score += 0.5 if response.count(config['solution_end']) == 1 else -1.0
                    scores.append(score)
                return scores

            def check_answer(prompts, completions, answer, **kwargs):
                responses = [completion[0]["content"] for completion in completions]
                extracted_responses = [
                    guess.group(1) if (guess := match_format.search(r)) is not None else None
                    for r in responses
                ]
                scores = []
                for guess, true_answer in zip(extracted_responses, answer):
                    score = 0
                    if guess is None:
                        scores.append(-2.0)
                        continue
                    if guess == true_answer:
                        score += 5.0
                    elif guess.strip() == true_answer.strip():
                        score += 3.5
                    else:
                        try:
                            ratio = float(guess) / float(true_answer)
                            if 0.9 <= ratio <= 1.1:
                                score += 2.0
                            elif 0.8 <= ratio <= 1.2:
                                score += 1.5
                            else:
                                score -= 2.5
                        except:
                            score -= 4.5
                    scores.append(score)
                return scores
            
            global PRINTED_TIMES
            PRINTED_TIMES = 0
            global PRINT_EVERY_STEPS
            PRINT_EVERY_STEPS = 5

            def check_numbers(prompts, completions, answer, **kwargs):
                global PRINTED_TIMES
                question = prompts[0][-1]["content"]
                responses = [completion[0]["content"] for completion in completions]
                extracted_responses = [
                    guess.group(1) if (guess := match_numbers.search(r)) is not None else None
                    for r in responses
                ]
                if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:
                    print(
                        '*'*20 + f"Question:\n{question}", f"\nAnswer:\n{answer[0]}", f"\nResponse:\n{responses[0]}", f"\nExtracted:\n{extracted_responses[0]}"
                    )
                PRINTED_TIMES += 1
                scores = []
                for guess, true_answer in zip(extracted_responses, answer):
                    if guess is None:
                        scores.append(-2.5)
                        continue
                    try:
                        true_answer = float(true_answer.strip())
                        guess = float(guess.strip().replace(",", ""))
                        scores.append(3.5 if guess == true_answer else -1.5)
                    except:
                        scores.append(0)
                        continue
                return scores

            # Filter dataset
            tokenized = dataset.map(
                lambda x: {"tokens": tokenizer.apply_chat_template(x["prompt"], add_generation_prompt=True, tokenize=True)},
                batched=True,
            )
            tokenized = tokenized.map(lambda x: {"L": len(x["tokens"])})
            maximum_length = int(np.quantile(tokenized["L"], 0.9))
            dataset = dataset.select(np.where(np.array(tokenized["L"]) <= maximum_length)[0])

            # GRPO Trainer setup
            max_prompt_length = maximum_length + 1
            max_completion_length = config["max_seq_length"] - max_prompt_length

            vllm_sampling_params = SamplingParams(
                min_p=0.1,
                top_p=1.0,
                top_k=-1,
                seed=3407,
                stop=[tokenizer.eos_token],
                include_stop_str_in_output=True,
            )

            training_args = GRPOConfig(
                vllm_sampling_params=vllm_sampling_params,
                temperature=1.0,
                learning_rate=5e-6,
                weight_decay=0.001,
                warmup_ratio=0.1,
                lr_scheduler_type="linear",
                optim="adamw_8bit",
                logging_steps=1,
                per_device_train_batch_size=1,
                gradient_accumulation_steps=1,
                num_generations=4,
                max_prompt_length=max_prompt_length,
                max_completion_length=max_completion_length,
                max_steps=100,
                save_steps=100,
                report_to="none",
                output_dir="outputs",
            )

            trainer = GRPOTrainer(
                model=model,
                processing_class=tokenizer,
                reward_funcs=[
                    match_format_exactly,
                    match_format_approximately,
                    check_answer,
                    check_numbers,
                ],
                args=training_args,
                train_dataset=dataset,
            )

            trainer.train()
            
            os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
            model.save_lora(args.trained_model)
            print(f"Saved trained model to {args.trained_model}")

        if __name__ == '__main__':
            main()
    args:
      - --config
      - {inputValue: config}
      - --dataset
      - {inputPath: dataset}
      - --trained_model
      - {outputPath: trained_model}