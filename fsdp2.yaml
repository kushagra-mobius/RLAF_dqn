name: Train Model
description: Trains the RNN model.
outputs:
  - {name: random_result, type: String}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet kubeflow-training || \
        python3 -m pip install --quiet kubeflow-training --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        from kubeflow.training import TrainerClient
        from kubeflow.training.models import Trainer, RuntimeRef, PyTorch, Resources

        import os
        import argparse


        parser = argparse.ArgumentParser()
        parser.add_argument('--random_result_path', type=str, required=True)
        args = parser.parse_args()

        random_result_id = "test"
        os.makedirs(os.path.dirname(args.random_result_path), exist_ok=True)
        with open(args.random_result_path, 'w') as f:
            f.write(random_result_id)
        print(f"Random result ID {random_result_id} written to {args.random_result_path}")

        TRAIN_CODE = 'import os\\nimport torch\\nimport torch.distributed as dist\\nfrom torch.distributed.fsdp import FSDP\\nfrom torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\\nfrom torch.nn import Linear\\nfrom torch.optim import AdamW\\n\\ndef setup():\\n    dist.init_process_group("nccl")\\n    torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))\\n\\nclass ToyModel(torch.nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.net = Linear(1024, 1024)\\n\\n    def forward(self, x):\\n        return self.net(x)\\n\\ndef main():\\n    setup()\\n\\n    model = ToyModel().cuda()\\n    model = FSDP(\\n        model,\\n        auto_wrap_policy=size_based_auto_wrap_policy,\\n        device_id=torch.cuda.current_device(),\\n    )\\n\\n    opt = AdamW(model.parameters(), lr=1e-4)\\n\\n    for step in range(50):\\n        x = torch.randn(32, 1024, device="cuda")\\n        loss = model(x).sum()\\n        loss.backward()\\n        opt.step()\\n        opt.zero_grad()\\n\\n        if dist.get_rank() == 0:\\n            print(f"step={step}, loss={loss.item()}")\\n\\n    dist.destroy_process_group()\\n\\nif __name__ == "__main__":\\n    main()'

        client = TrainerClient(namespace="default")

        trainer = Trainer(
            name="fsdp2-single-script",
            runtime_ref=RuntimeRef(
                name="pytorch-distributed-runtime",
                kind="ClusterRuntime",
            ),
            trainer=PyTorch(
                num_workers=2,
                num_processes_per_worker=8,
                entrypoint="python",
                args=["-c", TRAIN_CODE],
                resources=Resources(
                    limits={
                        "nvidia.com/gpu": "8",
                        "cpu": "16",
                        "memory": "64Gi",
                    }
                ),
                env={
                    "NCCL_DEBUG": "INFO",
                    "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                },
            ),
        )

        client.create(trainer)
    
    args:
      - --random_result
      - {outputPath: random_result}
